{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b799085",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection — 01 Exploration & Preprocessing\n",
    "\n",
    "In this notebook, I:\n",
    "\n",
    "- Configure the Python environment and project paths  \n",
    "- Load the credit card fraud dataset  \n",
    "- Explore the structure and class imbalance  \n",
    "- Engineer a log-transformed transaction amount feature  \n",
    "- Split the data into training and test sets  \n",
    "- Scale features in preparation for model training\n",
    "\n",
    "Modeling and evaluation are done in a separate notebook: `02_modeling.ipynb`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef17d52e",
   "metadata": {},
   "source": [
    "## 1. Environment & Project Paths\n",
    "\n",
    "Before loading the dataset, I verify that the notebook is running under the correct Python interpreter and in the correct project directory. This protects against:\n",
    "\n",
    "- Missing packages  \n",
    "- Using the wrong virtual environment  \n",
    "- Incorrect file paths  \n",
    "- FileNotFound errors  \n",
    "\n",
    "I also confirm the dataset exists inside the expected `data/raw/` directory before continuing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5b2e0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Juant\\OneDrive\\Documents\\ML-Projects\\classic-ml-fraud-detection\\.venv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# DEBUGGING CHECK: Confirm the Python Interpreter in Use\n",
    "# ---------------------------------------------------------\n",
    "# Why this is important:\n",
    "# In VS Code, Jupyter notebooks sometimes run on the wrong\n",
    "# Python interpreter (e.g., the system Python instead of the\n",
    "# project's virtual environment `.venv`). This can cause:\n",
    "#   - Missing packages\n",
    "#   - Module import errors\n",
    "#   - Inconsistent environment behavior\n",
    "#\n",
    "# `sys.executable` prints the exact path of the Python\n",
    "# interpreter currently running this notebook. This helps\n",
    "# verify that the notebook is using the correct virtual\n",
    "# environment before continuing with the ML pipeline.\n",
    "\n",
    "import sys\n",
    "print(sys.executable)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90adfdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# STEP 1: Import Core Libraries\n",
    "# ---------------------------\n",
    "# NumPy: numerical operations\n",
    "# Pandas: data loading & manipulation\n",
    "# Matplotlib/Seaborn: visualization\n",
    "# Scikit-learn: model building, preprocessing, and evaluation\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7a65a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# DATA VISUALIZATION LIBRARY: Seaborn\n",
    "# ---------------------------------------------------------\n",
    "# Why we import Seaborn:\n",
    "# Seaborn is a statistical data visualization library built\n",
    "# on top of Matplotlib. It provides cleaner, more intuitive\n",
    "# plotting functions and visually appealing default styles.\n",
    "#\n",
    "# In this fraud detection project, Seaborn helps us:\n",
    "#   - Visualize class imbalance\n",
    "#   - Plot feature distributions (e.g., Amount vs. log_amount)\n",
    "#   - Create heatmaps and correlation matrices\n",
    "#   - Generate confusion matrices and ROC/PR curves with\n",
    "#     improved styling\n",
    "#\n",
    "# Overall, Seaborn makes it easier to understand the data,\n",
    "# communicate insights, and present findings in a way that\n",
    "# looks professional in a portfolio setting.\n",
    "\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f8c01f",
   "metadata": {},
   "source": [
    "## 2. Load the Dataset\n",
    "\n",
    "Here I load the Credit Card Fraud Detection dataset using Pandas.  \n",
    "This dataset contains anonymized PCA-transformed transaction features (`V1`–`V28`), the raw transaction amount, and the `Class` label:\n",
    "\n",
    "- **0 — legitimate transaction**  \n",
    "- **1 — fraudulent transaction**\n",
    "\n",
    "I begin by reading the dataset into a Pandas DataFrame and previewing the first few rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5001bd33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ---------------------------\n",
    "# STEP 2: Load the Dataset\n",
    "# ---------------------------\n",
    "# We load the well-known Credit Card Fraud Detection dataset.\n",
    "# This dataset is highly imbalanced and contains numerical features\n",
    "# obtained from PCA transformation, along with 'Amount' and 'Class'.\n",
    "# 'Class' = 1 indicates fraud, 'Class' = 0 indicates a normal transaction.\n",
    "\n",
    "df = pd.read_csv(\"data/raw/creditcard.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07373f5e",
   "metadata": {},
   "source": [
    "## 3. Dataset Structure & Class Imbalance\n",
    "\n",
    "Next, I inspect the dataset’s structure and verify the degree of class imbalance.  \n",
    "Fraud detection datasets are usually extremely imbalanced, which strongly affects model choice, evaluation strategy, and how thresholds are chosen in production.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7167e7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    284807 non-null  float64\n",
      " 1   V1      284807 non-null  float64\n",
      " 2   V2      284807 non-null  float64\n",
      " 3   V3      284807 non-null  float64\n",
      " 4   V4      284807 non-null  float64\n",
      " 5   V5      284807 non-null  float64\n",
      " 6   V6      284807 non-null  float64\n",
      " 7   V7      284807 non-null  float64\n",
      " 8   V8      284807 non-null  float64\n",
      " 9   V9      284807 non-null  float64\n",
      " 10  V10     284807 non-null  float64\n",
      " 11  V11     284807 non-null  float64\n",
      " 12  V12     284807 non-null  float64\n",
      " 13  V13     284807 non-null  float64\n",
      " 14  V14     284807 non-null  float64\n",
      " 15  V15     284807 non-null  float64\n",
      " 16  V16     284807 non-null  float64\n",
      " 17  V17     284807 non-null  float64\n",
      " 18  V18     284807 non-null  float64\n",
      " 19  V19     284807 non-null  float64\n",
      " 20  V20     284807 non-null  float64\n",
      " 21  V21     284807 non-null  float64\n",
      " 22  V22     284807 non-null  float64\n",
      " 23  V23     284807 non-null  float64\n",
      " 24  V24     284807 non-null  float64\n",
      " 25  V25     284807 non-null  float64\n",
      " 26  V26     284807 non-null  float64\n",
      " 27  V27     284807 non-null  float64\n",
      " 28  V28     284807 non-null  float64\n",
      " 29  Amount  284807 non-null  float64\n",
      " 30  Class   284807 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Class\n",
       "0    284315\n",
       "1       492\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# STEP 3: Inspect the Data\n",
    "# ---------------------------\n",
    "# Check the structure, column types, and confirm that the dataset loaded correctly.\n",
    "\n",
    "df.info()\n",
    "\n",
    "# Check class distribution (fraud vs non-fraud)\n",
    "# Fraud cases are extremely rare — less than 0.2%.\n",
    "df['Class'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220cd88b",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "The `Amount` feature is highly skewed, with many small transactions and a few very large ones.  \n",
    "To help machine learning models learn more effectively, I create a log-transformed version of this feature (`log_amount`). This transformation reduces skew and compresses outliers, while preserving important relative differences between transaction amounts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b201ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Amount</th>\n",
       "      <th>log_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>149.62</td>\n",
       "      <td>5.014760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.69</td>\n",
       "      <td>1.305626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>378.66</td>\n",
       "      <td>5.939276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>123.50</td>\n",
       "      <td>4.824306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69.99</td>\n",
       "      <td>4.262539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Amount  log_amount\n",
       "0  149.62    5.014760\n",
       "1    2.69    1.305626\n",
       "2  378.66    5.939276\n",
       "3  123.50    4.824306\n",
       "4   69.99    4.262539"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# STEP 4: Feature Engineering — Log Transform the 'Amount' Feature\n",
    "# ---------------------------\n",
    "# Why log transform?\n",
    "# - Transaction amounts are highly skewed (long tail distribution)\n",
    "# - Skewed features can negatively impact ML models\n",
    "# - Log transformation compresses large values and expands small ones,\n",
    "#   making patterns easier to learn.\n",
    "# We keep both 'Amount' and 'log_amount' because different model families\n",
    "# benefit from different representations.\n",
    "\n",
    "df['log_amount'] = np.log1p(df['Amount'])   # log1p handles zero values safely\n",
    "df[['Amount', 'log_amount']].head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc54e5b",
   "metadata": {},
   "source": [
    "## 5. Train/Test Split & Feature Scaling\n",
    "\n",
    "To fairly evaluate model performance, I split the dataset into training and test sets using a stratified split to preserve the fraud ratio.  \n",
    "I then standardize the features using `StandardScaler`, fitting the scaler only on the training data to prevent data leakage and applying the same transformation to the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d52e06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (227845, 31)\n",
      "Test shape: (56962, 31)\n",
      "Fraud ratio in train: 0.001729245759178389\n",
      "Fraud ratio in test: 0.0017204452090867595\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# STEP: Split the Dataset into Training and Testing Sets\n",
    "# ---------------------------------------------------------\n",
    "# - We separate features (X) and labels (y) above.\n",
    "# - Now we split them into:\n",
    "#       * Training set  → used to fit the model\n",
    "#       * Testing set   → used to evaluate model performance\n",
    "#\n",
    "# Why this matters:\n",
    "# - Fraud datasets are extremely imbalanced, so we use\n",
    "#   stratify=y to preserve the fraud/non-fraud ratio.\n",
    "# - random_state=42 ensures the split is reproducible.\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate the dataset into input features (X) and target labels (y)\n",
    "X = df.drop(columns=['Class'])   # All feature columns\n",
    "y = df['Class']                  # Fraud label (0 or 1)\n",
    "\n",
    "# Perform the train/test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,      # 20% of data reserved for testing\n",
    "    stratify=y,         # Maintain fraud ratio across splits\n",
    "    random_state=42     # Ensures reproducibility\n",
    ")\n",
    "\n",
    "# Display shapes & basic fraud ratios to confirm correct splitting\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Test shape:\", X_test.shape)\n",
    "print(\"Fraud ratio in train:\", y_train.mean())\n",
    "print(\"Fraud ratio in test:\", y_test.mean())\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# STEP: Scale Numerical Features\n",
    "# ---------------------------------------------------------\n",
    "# Why scale?\n",
    "# - Logistic Regression, SVMs, and many ML algorithms perform\n",
    "#   better when numerical features share a similar scale.\n",
    "#\n",
    "# IMPORTANT:\n",
    "# - Fit the scaler ONLY on the training data to avoid \n",
    "#   data leakage.\n",
    "# - Apply the SAME scaler to transform the test set.\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on training data and transform it\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using the same scaling parameters\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbcf062",
   "metadata": {},
   "source": [
    "## 6. Save Preprocessed Data for Modeling\n",
    "\n",
    "To keep the workflow modular and reproducible, I save the preprocessed training and test sets to disk.  \n",
    "The modeling notebook (`02_modeling.ipynb`) will load these saved arrays instead of re-running all preprocessing steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6e88529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved preprocessed data to 'data/processed/'.\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# SAVE PREPROCESSED DATA FOR MODELING\n",
    "# ---------------------------------------------------------\n",
    "# Why save?\n",
    "# - Keeps preprocessing and modeling decoupled\n",
    "# - Allows the modeling notebook to run independently\n",
    "# - Mirrors real ML workflows where data prep and modeling\n",
    "#   happen in separate steps or services.\n",
    "#\n",
    "# We save:\n",
    "#   - X_train_scaled, X_test_scaled: input features\n",
    "#   - y_train, y_test: labels\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "# Ensure the processed data directory exists\n",
    "os.makedirs(\"data/processed\", exist_ok=True)\n",
    "\n",
    "# Save the preprocessed arrays\n",
    "joblib.dump(X_train_scaled, \"data/processed/X_train_scaled.pkl\")\n",
    "joblib.dump(X_test_scaled, \"data/processed/X_test_scaled.pkl\")\n",
    "joblib.dump(y_train, \"data/processed/y_train.pkl\")\n",
    "joblib.dump(y_test, \"data/processed/y_test.pkl\")\n",
    "\n",
    "print(\"Saved preprocessed data to 'data/processed/'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57b3de9",
   "metadata": {},
   "source": [
    "## 7. Next Steps\n",
    "\n",
    "With the data prepared, standardized, and split into training and test sets, the next steps will be performed in a separate modeling notebook:\n",
    "\n",
    "- Train a baseline Logistic Regression model  \n",
    "- Evaluate performance using imbalanced-data metrics  \n",
    "  (ROC AUC, Precision, Recall, F1-score, PR AUC)  \n",
    "- Visualize the ROC and Precision-Recall curves  \n",
    "- Train and compare a tree-based model such as XGBoost  \n",
    "- Discuss how threshold selection affects fraud detection performance  \n",
    "\n",
    "This concludes the preprocessing stage of the workflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571cef73",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
